---
title: "comparing means"
author: "Benta Ng'ang'a"
date: '2022-07-22'
output: html_document
---
one- sample t-test
independent
paired t-test
Anova
#H0:m=μ
#Ha:m≠μ
p value < 0.05 reject null hypothesis
p value > 0.05 fail to reject null hypothesis
#1. One sample t-test
one-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean (μ).
eg a student gets an average of 50 . 50 is x bar
Generally, the theoretical mean comes from:

a previous experiment. For example, compare whether the mean weight of mice differs from 200 mg, a value determined in a previous study.
or from an experiment where you have control and treatment conditions. If you express your data as “percent of control”, you can test whether the average value of treatment condition differs significantly from 100.
-Note that, one-sample t-test can be used only, when the data are normally distributed . This can be checked using Shapiro-Wilk test .
In statistics, we can define the corresponding null hypothesis (H0) as follow:

H0:m=μ
H0:m≤μ
H0:m≥μ
The corresponding alternative hypotheses (Ha) are as follow:

Ha:m≠μ (different)
Ha:m>μ (greater)
Ha:m<μ (less)
computing
Shapiro-Wilk test:
Null hypothesis: the data is normally distributed
Alternative hypothesis: the data is not normally distributed
if p value is greater than 0.05 we fail to reject H0 ,data is normally distributed
```{r}
set.seed(1234)
my_data <- data.frame(
  name = paste0(rep("M_", 10), 1:10),
  weight = round(rnorm(10, 20, 2), 1)
)
my_data
#We want to know, if the average weight of the mice differs from 25g (two-tailed test)?
shapiro.test(my_data$weight) # => p-value = 0.6993 greater than 0.05, Data is normally distributed
# One-sample t-test
res <- t.test(my_data$weight, mu = 25)
# Printing the results
res 
#t.test(x, mu = 0, alternative = "two.sided")
#H0:m=μ
#Ha:m≠μ
# p value 7.95310^{-6} < alpha(0.05) we reject the null hypothesis
#The p-value of the test is 7.95310^{-6}, which is less than the significance level alpha = 0.05. We can conclude that the mean weight of the mice is significantly different from 25g with a p-value = 7.95310^{-6}.

```
set.seed() function in R Language is used to create random numbers which can be reproduced. It helps in creating same random numbers each time a random function is called. This helps in creating repeatable data sets for a analysis.

Syntax: set.seed(n)

Parameters:
n: seeds for repeatable data sets

Example 1:
```{r}

# R program to create repeatable data sets
  
# Setting seeds
set.seed(10)
  
# Creating data set of random values
x <- rnorm(15)
x
  
# Setting seed again for another data set
set.seed(10)
  
# Creating another data set
y <- rnorm(15)
y
  
# Checking if both are equal
identical(x, y)

```

#2. Independent sample t-test
contains a continuous variable and a categorical variable (with only two levels)
H0: μ1 = μ2

Ha: μ1 ≠ μ2
p value in the output= p calculated
p calc < p tabulated(0.05) - reject H0
p calc > p tabulated(0.05) - fail to reject H0
 
assumptions
- samples are independent
- samples are drawn from populations with equal variances??
var.test(v~b)
H0: Variances are equal
Ha: Variances are not equal
if p value is greater than 0.05 we fail to reject H0 , variances are equal
- samples are drawn from populations with normal distributions
1.use shapiro test when you have 20-80 samples for better accuracy
2.normal qquantile plots
qqnorm(m$l)
qqline(m$l)
when the assumptions are not met use the non parametric method (wilcox.tes)
The code to run an independent-samples t-test using R is as follows:
```{r}
#t.test (dv ~ iv, var.equal=TRUE, data = dataframe)
```

#3. Paired sample t-test
The paired samples t-test is used to compare the means between two related groups of samples. In this case, you have two values (i.e., pair of values) for the same samples

t.test(x, y, paired = TRUE, alternative = "two.sided")

x,y: numeric vectors
paired: a logical value specifying that we want to compute a paired t-test
alternative: the alternative hypothesis. Allowed value is one of “two.sided” (default), “greater” or “less”.
 
Assumption 1: Independent variable consists of a "matched (non-independent) pair"
Assumption 2: the distribution of the differences in the independent variables between the matched pairs follow a normal distribution
```{r}
# Weight of the mice before treatment
before <-c(200.1, 190.9, 192.7, 213, 241.4, 196.9, 172.2, 185.5, 205.2, 193.7)
# Weight of the mice after treatment
after <-c(392.9, 393.2, 345.1, 393, 434, 427.9, 422, 383.9, 392.3, 352.2)
# Create a data frame
my_data <- data.frame( 
                group = rep(c("before", "after"), each = 10),
                weight = c(before,  after)
                )
my_data
library(dplyr)
group_by(my_data,group) %>% summarise(count= n (),mean = mean(weight, na.rm = TRUE),
    sd = sd(weight, na.rm = TRUE)
  )
library("ggpubr")
ggboxplot(my_data, x = "group", y = "weight", 
          color = "group", palette = c("#00AFBB", "#E7B800"),
          order = c("before", "after"),
          ylab = "Weight", xlab = "Groups")
```
```{r}
# Subset weight data before treatment
before <- subset(my_data,  group == "before", weight,
                 drop = F)
before
# subset weight data after treatment
after <- subset(my_data,  group == "after", weight,
                 drop = TRUE)
# Plot paired data
library(PairedData)
pd <- paired(before, after)
pd
plot(pd, type = "profile") + theme_bw()

```
Assumption 1: Are the two samples paired?

Yes, since the data have been collected from measuring twice the weight of the same mice.

Assumption 2: Is this a large sample?

No, because n < 30. Since the sample size is not large enough (less than 30), we need to check whether the differences of the pairs follow a normal distribution.

How to check the normality?

Use Shapiro-Wilk normality test as described at: Normality Test in R.

Null hypothesis: the data are normally distributed
Alternative hypothesis: the data are not normally distributed
```{r}
# compute the difference
d <- with(my_data, 
        weight[group == "before"] - weight[group == "after"])
# Shapiro-Wilk normality test for the differences
shapiro.test(d) # => p-value = 0.6141
```
From the output, the p-value is greater than the significance level 0.05 implying that the distribution of the differences (d) are not significantly different from normal distribution. In other words, we can assume the normality.

Note that, if the data are not normally distributed, it’s recommended to use the non parametric paired two-samples Wilcoxon test.
```{r}
res <- t.test(weight ~ group, data = my_data, paired = TRUE)
res
#The p-value of the test is 6.210^{-9}, which is less than the significance level alpha = 0.05. We can then reject null hypothesis and conclude that the average weight of the mice before treatment is significantly different from the average weight after treatment with a p-value = 6.210^{-9}.
# printing the p-value
res$p.value
# printing the mean
res$estimate
#mean of the differences 
```


#4. Anova
$$ https://www.scribbr.com/statistics/anova-in-r/ $$
used when having a continuous variable with a categorical variable with more than two levels
it is a parametric method appropriate for comparing the means for 2 or more independent populations

The null hypothesis (H0) of the ANOVA is no difference in means
Alternate hypothesis (Ha) is that the means are different from one another.
```{r}
crop<- read.csv("crop.csv")
crop
summary(crop)
#ANOVA tests whether any of the group means are different from the overall mean of the data by checking the variance of each individual group against the overall variance of the data. 

```
one way
```{r}
one.way <- aov(yield ~ fertilizer, data = crop)

summary(one.way)
#(1,94)=14.91,p < .05
#p value < .05 
#it appears that the type of fertilizer used has a real impact on the final crop yield.
```
The Df column displays the degrees of freedom for the independent variable (the number of levels in the variable minus 1), and the degrees of freedom for the residuals (the total number of observations minus one and minus the number of levels in the independent variables).

The Sum Sq column displays the sum of squares (a.k.a. the total variation between the group means and the overall mean).

The Mean Sq column is the mean of the sum of squares, calculated by dividing the sum of squares by the degrees of freedom for each parameter.

The F-value column is the test statistic from the F test. This is the mean square of each independent variable divided by the mean square of the residuals. The larger the F value, the more likely it is that the variation caused by the independent variable is real and not due to chance.

The Pr(>F) column is the p-value of the F-statistic. This shows how likely it is that the F-value calculated from the test would have occurred if the null hypothesis of no difference among group means were true.

two way
```{r}
two.way <- aov(yield ~ fertilizer + density, data = crop)

summary(two.way)
#Adding planting density to the model seems to have made the model better: it reduced the residual variance (the residual sum of squares went from 36.21 to 31.089), and both planting density and fertilizer are statistically significant (p-values < 0.001).
```
Adding interactions between variables
Sometimes you have reason to think that two of your independent variables have an interaction effect rather than an additive effect.

For example, in our crop yield experiment, it is possible that planting density affects the plants’ ability to take up fertilizer. This might influence the effect of fertilizer type in a way that isn’t accounted for in the two-way model.

To test whether two variables have an interaction effect in ANOVA, simply use an asterisk instead of a plus-sign in the model:
```{r}
interaction <- aov(yield ~ fertilizer*density, data = crop)

summary(interaction)

```
in the output table, the ‘fertilizer:density’ variable has a low sum-of-squares value and a high p-value, which means there is not much variation that can be explained by the interaction between fertilizer and planting density.??
Adding a blocking variable
```{r}
blocking <- aov(yield ~ fertilizer + density + block, data = crop)

summary(blocking)
```

Step 3: Find the best-fit model??
```{r}

library(AICcmodavg)
model.set <- list(one.way, two.way, interaction, blocking)
model.names <- c("one.way", "two.way", "interaction", "blocking")

aictab(model.set, modnames = model.names)
```
The model with the lowest AIC score (listed first in the table) is the best fit for the data:

AIC model selection
From these results, it appears that the two.way model is the best fit. The two-way model has the lowest AIC value, and 71% of the AIC weight, which means that it explains 71% of the total variation in the dependent variable that can be explained by the full set of models.

The model with blocking term contains an additional 15% of the AIC weight, but because it is more than 2 delta-AIC worse than the best model, it probably isn’t good enough to include in your results.




#Example 2
levels of stress
group 1 -normal
group 2 -after layoffs
group 3 -during layoffs
```{r}
group1<-c(2,3,7,2,6)
group2<-c(10,8,7,5,10)
group3<-c(10,13,14,13,15)
dat<- data.frame(cbind(group1,group2,group3))
dat
stacked.groups<-stack(dat)
stacked.groups
anova_res<- aov(values~ind,data= stacked.groups)
anova_res
summary(anova_res)
#F(2,12)=2.59,p<.05
```
#5. Chi square test
Square test in R is a statistical method which used to determine if two categorical variables have a significant correlation between them. The two variables are selected from the same population.

syntax
chisq.test(data)

we reject the null hypothesis if the p-value is less  than a predetermined significance level, which is 0.05 usually, then we reject the null hypothesis.(p value <.05)

H0: The two variables are independent.
1: The two variables relate to each other.
```{r}

data_frame <- read.csv("https://goo.gl/j6lRXD")#Reading CSV 
data_frame
table(data_frame$treatment, data_frame$improvement)
chisq.test(data_frame$treatment, data_frame$improvement, correct=F)
#We have a chi-squared value of 5.5569. Since we get a p-Value less than the significance level of 0.05, we reject the null hypothesis and conclude that the two variables are in fact dependent.
```

#6. Cross tabulation
Cross-tabulation for a pair of categorical variables with either row, column, or total proportions, as well as marginal sums. Works with numeric, character, as well as factor variables.
